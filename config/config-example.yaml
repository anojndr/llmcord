# ==============================================================================
# Discord Bot Settings
# ==============================================================================

bot_token: YOUR_DISCORD_BOT_TOKEN # Replace with your bot's token
client_id: YOUR_DISCORD_CLIENT_ID # Replace with your bot's client ID
status_message: "Listening to your commands!" # Message displayed as the bot's status

# --- Message & Interaction Configuration ---
# max_text: 2000       # This is replaced by max_text_limits below.
max_text_limits:
  default: 128000 # Default max_text in tokens for LLM prompt (system, history, content, images). Uses tiktoken.
  models:
    "openai/gpt-4.1": 128000
    "google/gemini-2.5-flash-preview-05-20": 250000
    "anthropic/claude-3.7-sonnet": 90000
    "anthropic/claude-3.7-sonnet-thought": 90000
    "anthropic/claude-sonnet-4": 80000
max_text_safety_margin: 5000 # Tokens to subtract from max_text_limits for a safety buffer.
min_token_limit_after_safety_margin: 1000 # Minimum token limit after applying the safety margin.
max_images: 5        # Max number of images allowed in a single message.
max_messages: 25     # Max number of messages to include in the history sent to the LLM.
max_message_node_cache: 500 # Max messages stored in the internal bot cache for quick history retrieval.

# --- Behavior Settings ---
edit_delay_seconds: 1.0 # Delay in seconds between edits for streaming responses.
use_plain_responses: false # If true, bot responses will be plain text without Discord formatting.
allow_dms: true          # If true, the bot will respond to direct messages.

# --- Permissions ---
# Configure user, role, and channel permissions for bot interaction.
# IDs are Discord Snowflakes (e.g., user ID, role ID, channel ID).
# Empty lists mean no restrictions of that type.
permissions:
  users:
    allowed_ids: [] # Whitelist: Only these user IDs can interact.
    blocked_ids: [] # Blacklist: These user IDs cannot interact.
  roles:
    allowed_ids: [] # Whitelist: Only users with these role IDs can interact.
    blocked_ids: [] # Blacklist: Users with these role IDs cannot interact.
  channels:
    allowed_ids: [] # Whitelist: Bot only operates in these channel IDs.
    blocked_ids: [] # Blacklist: Bot ignores these channel IDs.

# ==============================================================================
# API Keys & External Services
# ==============================================================================

# --- YouTube Data API v3 ---
# Required for fetching YouTube video transcripts and metadata.
# Get an API key from Google Cloud Console: https://console.cloud.google.com/apis/credentials
# Ensure the "YouTube Data API v3" is enabled for your project.
youtube_api_key: YOUR_YOUTUBE_API_KEY # NOTE: YouTube API still uses a single key in this implementation

# --- Reddit API ---
# Required for fetching content from Reddit.
# Create a 'script' app on Reddit: https://www.reddit.com/prefs/apps
reddit_client_id: YOUR_REDDIT_CLIENT_ID
reddit_client_secret: YOUR_REDDIT_CLIENT_SECRET
reddit_user_agent: "discord:your-llm-bot-name:v1.0 (by u/your_reddit_username)" # Customize appropriately

# --- SerpAPI ---
# For web search capabilities if not using a model with built-in search (e.g., non-Gemini/Grok).
# Get API keys from SerpApi: https://serpapi.com/manage-api-key
# Provide a list of keys for rotation and retries.
serpapi_api_keys:
  - YOUR_SERPAPI_KEY_1
  # Optional: Additional SerpAPI key for rotation. Comment out to disable.
  - YOUR_SERPAPI_KEY_2
  # Optional: Additional SerpAPI key for rotation. Comment out to disable.
  - ...

# --- Proxy Configuration for YouTube Transcripts (Optional) ---
# See: https://github.com/jdepoix/youtube-transcript-api#working-around-ip-bans-requestblocked-or-ipblocked-exception
# Uncomment and configure ONE of the types below if you experience transcript fetch errors.
# Optional: Proxy configuration for youtube-transcript-api. Configure as needed. Comment out the entire block to disable. Note: Only one proxy type (webshare or generic) should be configured if used.
proxy_config:
  type: "webshare" # Recommended for reliability. Requires a Webshare "Residential" proxy plan.
  # For webshare:
  username: "YOUR_WEBSHARE_USERNAME" # Find in your Webshare Proxy Settings
  password: "YOUR_WEBSHARE_PASSWORD" # Find in your Webshare Proxy Settings
# --- OR ---
# proxy_config:
#   type: "generic"
#   # For generic:
#   http_url: "http://user:pass@your_proxy_host:port" # Your HTTP proxy URL
#   https_url: "https://user:pass@your_proxy_host:port" # Your HTTPS proxy URL (often same as http_url)


# ==============================================================================
# LLM Provider Settings
# ==============================================================================

providers:
  openai:
    base_url: https://api.openai.com/v1
    api_keys:
      - YOUR_OPENAI_KEY_1
      # Optional: Additional OpenAI API key. Comment out to disable.
      - YOUR_OPENAI_KEY_2
      # Optional: Additional OpenAI API key. Comment out to disable.
      - ...
    disable_vision: false # Set to true to disable vision capabilities for OpenAI models.
  x-ai:
    base_url: https://api.x.ai/v1
    api_keys:
      - YOUR_XAI_KEY_1
      # Optional: Additional X-AI API key. Comment out to disable.
      - ...
  google:
    # For google-genai (Gemini), base_url is not used. API keys are configured directly.
    api_keys:
      - YOUR_GEMINI_KEY_1
      # Optional: Additional Google API key. Comment out to disable.
      - YOUR_GEMINI_KEY_2
      # Optional: Additional Google API key. Comment out to disable.
      - ...
  mistral:
    base_url: https://api.mistral.ai/v1
    api_keys:
      - YOUR_MISTRAL_KEY_1
      # Optional: Additional Mistral API key. Comment out to disable.
      - ...
  groq:
    base_url: https://api.groq.com/openai/v1
    api_keys:
      - YOUR_GROQ_KEY_1
      # Optional: Additional Groq API key. Comment out to disable.
      - ...
  openrouter:
    base_url: https://openrouter.ai/api/v1
    api_keys:
      - YOUR_OPENROUTER_KEY_1
      # Optional: Additional OpenRouter API key. Comment out to disable.
      - ...

  # --- Local / Keyless Providers ---
  # These providers typically run locally and do not require API keys.
  ollama:
    base_url: http://localhost:11434/v1 # Default Ollama API endpoint.
    api_keys: [] # Explicitly empty list for keyless providers.
  lmstudio:
    base_url: http://localhost:1234/v1 # Default LM Studio API endpoint.
    api_keys: []
  vllm:
    base_url: http://localhost:8000/v1 # Default vLLM API endpoint.
    api_keys: []
  oobabooga:
    base_url: http://localhost:5000/v1 # Default Oobabooga Text Generation WebUI API endpoint.
    api_keys: []
  jan:
    base_url: http://localhost:1337/v1 # Default Jan API endpoint.
    api_keys: []

# --- Default Model Selection ---
# Specify the primary model to use for responses. Format: "provider_name/model_identifier"
model: openai/gpt-4o # Example: Selects gpt-4o from OpenAI.

# Model used for the grounding pre-step (e.g., generating search queries for SearXNG).
grounding_model: "google/gemini-1.5-flash-latest"

# Fallback model if the selected 'model' doesn't support vision but images are present.
fallback_vision_model: "google/gemini-1.5-flash-latest"

# Fallback model if a non-Gemini stream ends incompletely or a 422 error occurs.
fallback_model_incomplete_stream: "google/gemini-1.5-flash-latest"

# Model to use when 'deepsearch' or 'deepersearch' keywords are detected in user input.
deep_search_model: "x-ai/grok-1" # Example, ensure this model is available and configured.

# Model to use for the /enhanceprompt command.
enhance_prompt_model: "google/gemini-1.5-flash-latest" # Example, can be any configured model.

# --- Grounding Model Specific Parameters (Primarily for Gemini) ---
# These parameters are applied if the 'grounding_model' is a Gemini model and supports them.
# They fine-tune how search queries are generated for grounding.

# Optional: Temperature for the grounding model. Affects randomness. Comment out to disable.
# Affects randomness. Higher values (e.g., 0.9) make output more random, lower (e.g., 0.2) more deterministic.
# Must be a float between 0.0 and 1.0. Default: 0.7 (see llmcord_app/constants.py)
grounding_model_temperature: 0.7

# Optional: Top-k sampling for the grounding model. Comment out to disable.
# The model will consider the k most probable tokens at each step.
# Must be a positive integer. Default: 40 (see llmcord_app/constants.py)
grounding_model_top_k: 40

# Optional: Top-p (nucleus) sampling for the grounding model. Comment out to disable.
# The model will consider the smallest set of tokens whose cumulative probability exceeds p.
# Must be a float between 0.0 and 1.0. Default: 0.95 (see llmcord_app/constants.py)
grounding_model_top_p: 0.95

# Optional: Enable thinking budget for the grounding model (if it's Gemini).
# This is separate from the main 'gemini_use_thinking_budget' for user-facing responses.
# Default: false (see llmcord_app/constants.py)
grounding_model_use_thinking_budget: true

# Optional: Value for the grounding model's thinking budget if enabled.
# (0-24576). Default: 0 (thinking disabled) (see llmcord_app/constants.py)
grounding_model_thinking_budget_value: 0

# --- Extra API Parameters ---
# These parameters are passed to the LLM API if supported by the provider and model.
# They allow for fine-tuning model behavior beyond standard configurations.
extra_api_parameters:
  # Common OpenAI-compatible parameters:
  max_tokens: 4096       # Maximum number of tokens to generate in the response.
  temperature: 1.0       # Sampling temperature. Higher values = more random, lower = more deterministic.
  # Optional: Top-p sampling for LLM. Comment out to disable.
  top_p: 1.0             # Nucleus sampling: considers tokens with cumulative probability mass p.
  # Optional: Frequency penalty for LLM. Comment out to disable.
  frequency_penalty: 0.0 # Penalizes new tokens based on their existing frequency in the text so far.
  # Optional: Presence penalty for LLM. Comment out to disable.
  presence_penalty: 0.0  # Penalizes new tokens based on whether they appear in the text so far.

  # Optional: Example Gemini-specific parameters. Configure as needed. Comment out the entire block to disable.
  # These would be used if 'model' starts with 'google/' and the Gemini API supports them.
  # Refer to Google AI Gemini API documentation for supported parameters.
  # max_output_tokens: 4096 # (Gemini specific) Max tokens in the output.
  # temperature: 1.0        # (Gemini specific) Sampling temperature.
  # top_p: 0.95             # (Gemini specific) Nucleus sampling.
  # top_k: 40               # (Gemini specific) Top-k sampling.

# --- System Prompts ---
# Default system prompt guiding the main LLM's behavior.
system_prompt: |
  You are a helpful assistant. When answering questions involving mathematics, statistics, or equations:

  1. Format mathematical expressions using LaTeX:
    - For inline math (within a sentence), use $...$ delimiters: $E=mc^2$
    - For display math (centered on its own line), use $$...$$ delimiters: $$\int_{a}^{b} f(x) \, dx = F(b) - F(a)$$

  2. Always use proper LaTeX notation for:
    - Fractions: $\frac{numerator}{denominator}$
    - Exponents: $x^{exponent}$ or $e^{-x^2}$
    - Subscripts: $x_{subscript}$
    - Greek letters: $\alpha$, $\beta$, $\pi$, etc.
    - Special functions: $\sin(x)$, $\log(x)$, $\lim_{x \to \infty}$
    - Matrices, integrals, sums, and products
    - Mathematical symbols: $\approx$, $\geq$, $\in$, etc.

  3. For multi-step solutions or derivations, use display math for clarity.

  4. For complex equations with multiple lines, use the align environment:
    $$\begin{align}
    y &= mx + b\\
    &= 2x + 3
    \end{align}$$

# System prompt for the 'grounding_model' (e.g., Gemini) when generating search queries
# for non-Gemini/non-Grok models. Guides how search terms are formulated.
grounding_system_prompt: |
  ANSWER IN ONE SENTENCE.

# Optional: System prompt for the fallback model. Comment out to disable.
# Used when a non-Gemini stream is incomplete or a 422 error occurs.
# If commented out or empty, a default prompt emphasizing brevity will be used.
fallback_model_system_prompt: |
  You are a very concise assistant. The previous model failed. Briefly answer the user's query.
 
# Optional: System prompt for the /enhanceprompt command.
# If commented out or empty, a default prompt will be used.
prompt_enhancer_system_prompt: |
  You are an expert prompt engineer. Your task is to refine a user's input to make it a more effective prompt for a large language model.
  Follow the provided prompt design strategies and guides to improve the user's original prompt.
  Output *only* the improved prompt, without any preamble, explanation, or markdown formatting.

# --- Rate Limiting ---
# Cooldown period in hours for API keys that hit a rate limit.
rate_limit_cooldown_hours: 24

# ==============================================================================
# Content Fetching, Grounding & LLM Behavior
# ==============================================================================

# --- SearXNG Settings ---
# SearXNG instance URL for grounding enhancement (web search results for non-Gemini/Grok models).
searxng_base_url: http://localhost:18088 # Your SearXNG instance with /search endpoint and JSON format enabled.

# Max character length for text extracted from each URL.
# - When web_content_extraction_api_enabled is false: Used for client-side content truncation in traditional SearXNG grounding.
# - When web_content_extraction_api_enabled is true: Sent as 'max_char_per_url' parameter to the API for server-side content limiting.
searxng_url_content_max_length: 20000
# Number of results to fetch from SearXNG for each grounding query.
searxng_num_results_fetch: 5

# --- General URL Content Extraction ---
# Primary and fallback methods for extracting content from web URLs.
# "crawl4ai": Advanced crawler for modern/JS-heavy sites.
# "beautifulsoup": Simpler HTML parser, good for basic sites or as fallback.
# "jina": Uses Jina Reader (r.jina.ai), good for general articles.
main_general_url_content_extractor: "crawl4ai"
fallback_general_url_content_extractor: "beautifulsoup"

# --- Jina Reader Settings (r.jina.ai) ---
# Engine mode for Jina Reader.
# "direct": Fastest, speed-first, no JS rendering. (X-Engine: direct)
# "browser": Best quality, handles JS, slower. (X-Engine: browser)
# "default": Jina's default, balances quality/speed. (No X-Engine header)
jina_engine_mode: "default"
# Optional: CSS selector for Jina to wait for. Comment out to disable.
# Example: "#content", ".main-article". Default: null (disabled).
jina_wait_for_selector: null
# Optional: Timeout in seconds for Jina. Comment out to disable.
# Integer (e.g., 30). Jina waits this duration for dynamic content. Default: null (Jina's default).
jina_timeout: null

# --- Crawl4AI Settings ---
# Cache mode for Crawl4AI.
# "bypass": (Default) No caching. Always fetches live.
# "enabled": Uses cache if available & not expired (1-day TTL). Fetches live if not cached/expired.
# "refresh": Fetches live and updates cache.
# "only_refresh": Fetches live only if already in cache (to update it).
# Optional: Cache mode for Crawl4AI. Comment out to disable.
crawl4ai_cache_mode: "bypass"

# --- External Web Content API (Optional) ---
# If enabled, this API fetches content from SearXNG URLs with enhanced processing capabilities.
# Content from this API is appended to LLM query. URLs processed here are skipped by internal fetchers.
# When enabled, 'searxng_url_content_max_length' is sent as 'max_char_per_url' to the API for server-side content limiting.
web_content_extraction_api_enabled: false # Set to true to enable.
web_content_extraction_api_url: "http://localhost:8080/search" # URL of the API.
web_content_extraction_api_max_results: 3 # Max results to request from this API.

# --- Gemini Thinking Budget (Optional) ---
# Allows Gemini models more "thinking" time, potentially improving complex query responses but increasing latency.
# Applies ONLY to Gemini models. See Google AI docs for "thinkingBudget".
# Optional: Enable Gemini thinking budget. Comment out to disable.
# Users can override with /setgeminithinking command.
gemini_use_thinking_budget: false
# Optional: Value for Gemini thinking budget. Comment out to disable.
# Actual budget value (0-24576) if enabled. Global, not user-configurable by command.
gemini_thinking_budget_value: 1024

# --- Gemini Safety Settings ---
# Configure safety thresholds for Gemini models.
# Categories: HARM_CATEGORY_HARASSMENT, HARM_CATEGORY_HATE_SPEECH, HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT, HARM_CATEGORY_CIVIC_INTEGRITY.
# Thresholds: BLOCK_NONE, BLOCK_ONLY_HIGH, BLOCK_MEDIUM_AND_ABOVE, BLOCK_LOW_AND_ABOVE, HARM_BLOCK_THRESHOLD_UNSPECIFIED.
# Defaults vary by model version; check Google AI Gemini API docs.
# gemini-1.5-pro/flash (002+) often default to BLOCK_NONE.
gemini_safety_settings:
  HARM_CATEGORY_HARASSMENT: BLOCK_NONE
  HARM_CATEGORY_HATE_SPEECH: BLOCK_NONE
  HARM_CATEGORY_SEXUALLY_EXPLICIT: BLOCK_NONE
  HARM_CATEGORY_DANGEROUS_CONTENT: BLOCK_NONE
  HARM_CATEGORY_CIVIC_INTEGRITY: BLOCK_NONE # Example for a supported category

# ==============================================================================
# Output Sharing Settings (Optional)
# ==============================================================================
# Enables sharing LLM output via a public text.is URL.
output_sharing:
  textis_enabled: false # Set to true to enable sharing LLM output via text.is.
  # ngrok_authtoken: YOUR_NGROK_AUTHTOKEN # Removed
  # grip_port: 6419 # Removed
  # ngrok_static_domain: null # Removed
  # cleanup_on_shutdown: true # Removed
  url_shortener_enabled: false # Set to true to use a URL shortener for the text.is link.
  url_shortener_service: "tinyurl" # Currently supported: "tinyurl".
  # Optional: API key for URL shortener service. Comment out to disable.
  # For future services that might require an API key.
  url_shortener_api_key: null

alternative_search_query_generation:
  enabled: false
  # Prompt to send to non-Gemini models for search query generation.
  # {latest_query} will be replaced with the user's latest message.
  # Note: If images are attached to the user's message, they will be sent to the LLM with this prompt.
# Optional: System prompt for the search query generation model.
  # {current_date} will be replaced with the current date.
  # If commented out or empty, no system prompt will be used for this specific LLM call.
  search_query_generation_system_prompt: |
    You are an AI assistant that analyzes user queries and chat history to determine if a web search is necessary.
    If a web search is needed, you generate concise and effective search queries.
    Today's date is {current_date}.
  search_query_generation_prompt_template: |
    <task>
    Analyze the latest query to determine if it requires web search. Consider the chat history for context.
    </task>

    <criteria>
    Web search IS required when the query asks about:
    - Current events, news, or recent developments
    - Real-time information (prices, weather, stock data)
    - Specific facts that may have changed recently
    - Information about new products, services, or updates
    - People, places, or organizations where current status matters

    Web search is NOT required when the query asks about:
    - General knowledge or established facts
    - Conceptual explanations or definitions
    - Personal opinions or advice
    - Mathematical calculations or logic problems
    - Analysis of provided information
    </criteria>

    <instructions>
    1. Analyze the latest query in the context of the chat history
    2. Determine if web search is required based on the criteria above
    3. If web search is required, generate specific search queries that would find the needed information
    4. For queries with multiple distinct subjects, create separate search queries for each
    5. Return your response in the exact JSON format shown in the examples
    </instructions>

    <examples>
    <example>
    <chat_history>
    User: What is machine learning?
    Assistant: Machine learning is a subset of artificial intelligence...
    </chat_history>
    <latest_query>Can you explain deep learning?</latest_query>
    <output>
    {"web_search_required": false}
    </output>
    </example>

    <example>
    <chat_history>
    User: I'm interested in electric vehicles
    Assistant: Electric vehicles are becoming increasingly popular...
    </chat_history>
    <latest_query>What are Tesla's latest Model 3 prices and what new features did Apple announce for iPhone 15?</latest_query>
    <output>
    {
      "web_search_required": true,
      "search_queries": [
        "Tesla Model 3 prices 2024",
        "Apple iPhone 15 new features announcement"
      ]
    }
    </output>
    </example>

    <example>
    <chat_history>
    User: Tell me about climate change
    Assistant: Climate change refers to long-term shifts in global temperatures...
    </chat_history>
    <latest_query>What were the key outcomes of the latest UN climate summit?</latest_query>
    <output>
    {
      "web_search_required": true,
      "search_queries": ["UN climate summit latest outcomes 2024"]
    }
    </output>
    </example>
    </examples>

    <chat_history>
    {chat_history}
    </chat_history>

    <latest_query>
    {latest_query}
    </latest_query>

    <output_format>
    Return ONLY valid JSON in one of these formats:
    - If no search needed: {"web_search_required": false}
    - If search needed: {"web_search_required": true, "search_queries": ["query1", "query2", ...]}
    </output_format>

# ==============================================================================
# Chat History Persistence Settings
# ==============================================================================
# Configure which types of external content should be persisted in the chat history
# and re-sent to the LLM in subsequent turns.
# For the *current* message being processed, all fetched external content (user URLs,
# search results, Google Lens) will be sent to the LLM.
# These settings only affect what is included from *past* messages in the history.
stay_in_chat_history:
  user_provided_urls: true # Content from URLs directly provided by the user in their message.
  search_results: false    # Content derived from web searches (e.g., SearxNG results).
  google_lens: true        # Content from Google Lens image analysis.